{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-10T15:34:08.188954Z",
     "iopub.status.busy": "2024-03-10T15:34:08.188535Z",
     "iopub.status.idle": "2024-03-10T15:34:09.418167Z",
     "shell.execute_reply": "2024-03-10T15:34:09.416952Z",
     "shell.execute_reply.started": "2024-03-10T15:34:08.188925Z"
    },
    "id": "7Egc8Dt3qlFo",
    "outputId": "37145cca-b647-46ea-e840-59c6b11738d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/ir-dataset/data-scrap.pickle\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:34:09.420347Z",
     "iopub.status.busy": "2024-03-10T15:34:09.419912Z",
     "iopub.status.idle": "2024-03-10T15:34:09.426446Z",
     "shell.execute_reply": "2024-03-10T15:34:09.425655Z",
     "shell.execute_reply.started": "2024-03-10T15:34:09.420318Z"
    },
    "id": "qG7jk2PSqlFr"
   },
   "outputs": [],
   "source": [
    "#data preprocessing will be going on here, bwlo mentioned are the key points:\n",
    "\n",
    "#taget of removal finding and removing similar symptoms\n",
    "#stop words removal\n",
    "#tokenisation\n",
    "#lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:34:39.886295Z",
     "iopub.status.busy": "2024-03-10T15:34:39.885872Z",
     "iopub.status.idle": "2024-03-10T15:34:41.239112Z",
     "shell.execute_reply": "2024-03-10T15:34:41.237804Z",
     "shell.execute_reply.started": "2024-03-10T15:34:39.886263Z"
    },
    "id": "RPMNuYXKqlFr"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from itertools import combinations\n",
    "from time import time\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:36:11.093179Z",
     "iopub.status.busy": "2024-03-10T15:36:11.092718Z",
     "iopub.status.idle": "2024-03-10T15:36:11.104367Z",
     "shell.execute_reply": "2024-03-10T15:36:11.103178Z",
     "shell.execute_reply.started": "2024-03-10T15:36:11.093146Z"
    },
    "id": "Opai9Zc9qlFs"
   },
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "splitter = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "with open('/kaggle/input/ir-dataset/data-scrap.pickle', 'rb') as handle:\n",
    "    dis_symp = pickle.load(handle)\n",
    "\n",
    "t0=time()\n",
    "total_symptoms = set() # Stores all unique symptoms\n",
    "diseases_symptoms_cleaned = OrderedDict() # Key: disease, Value:[List of symptoms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:36:11.322484Z",
     "iopub.status.busy": "2024-03-10T15:36:11.322071Z",
     "iopub.status.idle": "2024-03-10T15:36:11.327800Z",
     "shell.execute_reply": "2024-03-10T15:36:11.326730Z",
     "shell.execute_reply.started": "2024-03-10T15:36:11.322449Z"
    },
    "id": "wAnUAxG_qlFs",
    "outputId": "cf610606-319f-474e-a537-40421ac9a07b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "545\n"
     ]
    }
   ],
   "source": [
    "size_of_dict = len(dis_symp)\n",
    "print(size_of_dict)\n",
    "#for key, value in selected_dict.items():\n",
    " #   print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:36:39.947179Z",
     "iopub.status.busy": "2024-03-10T15:36:39.946718Z",
     "iopub.status.idle": "2024-03-10T15:36:40.135303Z",
     "shell.execute_reply": "2024-03-10T15:36:40.134208Z",
     "shell.execute_reply.started": "2024-03-10T15:36:39.947147Z"
    },
    "id": "vESgpDD3qlFt",
    "outputId": "667870f1-262b-4d75-95e3-436faca845ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /usr/share/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Set the NLTK_DATA environment variable\n",
    "os.environ[\"NLTK_DATA\"] = \"/path/to/nltk_data\"\n",
    "\n",
    "# Download the WordNet data\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:36:46.404082Z",
     "iopub.status.busy": "2024-03-10T15:36:46.403639Z",
     "iopub.status.idle": "2024-03-10T15:36:52.254235Z",
     "shell.execute_reply": "2024-03-10T15:36:52.253093Z",
     "shell.execute_reply.started": "2024-03-10T15:36:46.404050Z"
    },
    "id": "jx-D8oazqlFt"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:37:02.736638Z",
     "iopub.status.busy": "2024-03-10T15:37:02.735987Z",
     "iopub.status.idle": "2024-03-10T15:37:15.918860Z",
     "shell.execute_reply": "2024-03-10T15:37:15.917998Z",
     "shell.execute_reply.started": "2024-03-10T15:37:02.736599Z"
    },
    "id": "nizPyxwIqlFu"
   },
   "outputs": [],
   "source": [
    "# Iterate over all disease and preprocess symptoms string and break it into individual symptom\n",
    "for key in sorted(dis_symp.keys()):\n",
    "    value = dis_symp[key]\n",
    "    list_sym = re.sub(r\"\\[\\S+\\]\", \"\", value).lower().split(',')\n",
    "    #list_sym = [item for item in list_sym if item != '']\n",
    "    temp_sym = list_sym\n",
    "    list_sym = []\n",
    "    for sym in temp_sym:\n",
    "        if len(sym.strip())>0:\n",
    "            list_sym.append(sym.strip())\n",
    "    if \"none\" in list_sym:\n",
    "        list_sym.remove(\"none\");\n",
    "    if len(list_sym)==0:\n",
    "        continue\n",
    "    temp = list()\n",
    "    for sym in list_sym:\n",
    "        sym=sym.replace('-',' ')\n",
    "        sym=sym.replace(\"'\",'')\n",
    "        sym=sym.replace('(','')\n",
    "        sym=sym.replace(')','')\n",
    "        doc = nlp(sym)\n",
    "        processed_sym = ' '.join([token.lemma_ for token in doc if not token.is_stop and not token.is_digit])\n",
    "        total_symptoms.add(sym)\n",
    "        temp.append(sym)\n",
    "    diseases_symptoms_cleaned[key] = temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:37:25.787120Z",
     "iopub.status.busy": "2024-03-10T15:37:25.786338Z",
     "iopub.status.idle": "2024-03-10T15:37:25.793718Z",
     "shell.execute_reply": "2024-03-10T15:37:25.792501Z",
     "shell.execute_reply.started": "2024-03-10T15:37:25.787080Z"
    },
    "id": "wSEf8WP9qlFu"
   },
   "outputs": [],
   "source": [
    "total_symptoms = list(total_symptoms)\n",
    "total_symptoms.sort()\n",
    "total_symptoms=['label_dis']+total_symptoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:37:53.333337Z",
     "iopub.status.busy": "2024-03-10T15:37:53.332890Z",
     "iopub.status.idle": "2024-03-10T15:37:53.339649Z",
     "shell.execute_reply": "2024-03-10T15:37:53.338583Z",
     "shell.execute_reply.started": "2024-03-10T15:37:53.333307Z"
    },
    "id": "kopP6aKdqlFu",
    "outputId": "8d9d0486-3874-412d-d444-99db1a890d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "543\n"
     ]
    }
   ],
   "source": [
    "print(len(diseases_symptoms_cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:37:59.523696Z",
     "iopub.status.busy": "2024-03-10T15:37:59.520831Z",
     "iopub.status.idle": "2024-03-10T15:37:59.622696Z",
     "shell.execute_reply": "2024-03-10T15:37:59.621510Z",
     "shell.execute_reply.started": "2024-03-10T15:37:59.523654Z"
    },
    "id": "Wv3GexB6qlFu"
   },
   "outputs": [],
   "source": [
    "# Initialize two dataframes, one for normal dataset and one for combination dataset\n",
    "df_comb = pd.DataFrame(columns=total_symptoms)\n",
    "df_norm = pd.DataFrame(columns=total_symptoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:38:11.097990Z",
     "iopub.status.busy": "2024-03-10T15:38:11.097596Z",
     "iopub.status.idle": "2024-03-10T15:38:21.106793Z",
     "shell.execute_reply": "2024-03-10T15:38:21.105180Z",
     "shell.execute_reply.started": "2024-03-10T15:38:11.097962Z"
    },
    "id": "kenFNGOwqlFu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import combinations\n",
    "\n",
    "# Assuming df_norm and df_comb are initialized DataFrame objects\n",
    "\n",
    "# Read each disease and symptom list, convert into dictionary and add to dataframe\n",
    "for key, values in diseases_symptoms_cleaned.items():\n",
    "    key = str.encode(key).decode('utf-8')\n",
    "\n",
    "    # Populate row for normal\n",
    "    row_norm = dict({x: 0 for x in total_symptoms})\n",
    "    for sym in values:\n",
    "        row_norm[sym] = 1\n",
    "    row_norm['label_dis'] = key\n",
    "    df_norm = pd.concat([df_norm, pd.DataFrame([row_norm])], ignore_index=True)\n",
    "\n",
    "    # Populate rows for combination dataset\n",
    "    for comb in range(1, len(values) + 1):\n",
    "        for subset in combinations(values, comb):\n",
    "            row_comb = dict({x: 0 for x in total_symptoms})\n",
    "            for sym in list(subset):\n",
    "                row_comb[sym] = 1\n",
    "            row_comb['label_dis'] = key\n",
    "            df_comb = pd.concat([df_comb, pd.DataFrame([row_comb])], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-10T15:38:50.803455Z",
     "iopub.status.busy": "2024-03-10T15:38:50.802965Z",
     "iopub.status.idle": "2024-03-10T15:38:50.980246Z",
     "shell.execute_reply": "2024-03-10T15:38:50.978989Z",
     "shell.execute_reply.started": "2024-03-10T15:38:50.803386Z"
    },
    "id": "sFXhl20xqlFv"
   },
   "outputs": [],
   "source": [
    "df_comb.to_csv(\"Combined_Symptom.csv\",index=None)\n",
    "df_norm.to_csv(\"Individual_symptom.csv\",index=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UoMuiPiwqlFv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4573227,
     "sourceId": 7808595,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
